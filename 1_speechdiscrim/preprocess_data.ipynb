{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# For setting dataset cache dir if default is not desired\n",
    "import os\n",
    "os.environ['HF_HOME'] = r'D:\\hf_cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a36763bf70d4c9a971a2ae0f27ed3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5788ffdf9e19461aa3979f600a21f28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Download the audio dataset\n",
    "from datasets import load_dataset, Audio\n",
    "dataset = load_dataset(\"synthbot/pony-speech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['speaker', 'source', 'start', 'end', 'style', 'noise', 'transcription', 'audio', 'duration'],\n",
      "        num_rows: 64735\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 21:40:54 | INFO | fairseq.tasks.hubert_pretraining | current directory is D:\\Code\\raragan\\1_speechdiscrim\n",
      "2024-07-09 21:40:54 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'metadata', 'fine_tuning': False, 'labels': ['km'], 'label_dir': 'label', 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2024-07-09 21:40:54 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': True, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}\n"
     ]
    }
   ],
   "source": [
    "# 2. Extract speech features\n",
    "from svc_helper.sfeatures.models import RVCHubertModel\n",
    "sfeatures_model = RVCHubertModel(device=torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6b4bb833764bb6868165763a6584a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/64735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c1fee23bb840ddac08606603edda90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/64735 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3857748ae698414bb1dcb6de9979034b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/64734 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tqdm\n",
    "import librosa\n",
    "import torch\n",
    "from datasets import Array3D, Features\n",
    "\n",
    "def add_speech_features(example):\n",
    "    audio = example['audio']['array']\n",
    "    audio_resamp = librosa.resample(audio,\n",
    "        orig_sr=example['audio']['sampling_rate'],\n",
    "        target_sr=RVCHubertModel.expected_sample_rate)\n",
    "    feats = sfeatures_model.extract_features(audio=\n",
    "        torch.from_numpy(audio_resamp))\n",
    "    example['rvc_features'] = feats.cpu().numpy()\n",
    "    return example\n",
    "\n",
    "train_data = dataset['train']\n",
    "features = train_data.features.copy()\n",
    "features['rvc_features'] = Array3D(shape=(1, None, 768), dtype='float32')\n",
    "\n",
    "character_durations = {}\n",
    "character_counts = {}\n",
    "threshold_duration = 300\n",
    "\n",
    "def aggregate_durations(example):\n",
    "    if not example['speaker'] in character_durations:\n",
    "        character_durations[example['speaker']] = 0.0\n",
    "        character_counts[example['speaker']] = 0\n",
    "    character_durations[example['speaker']] += (example['end'] - example['start'])\n",
    "    character_counts[example['speaker']] += 1\n",
    "\n",
    "train_data.map(aggregate_durations)\n",
    "qualified_speakers = {speaker for speaker, duration in character_durations.items() if duration > threshold_duration}\n",
    "#dataset_with_features = train_data.shuffle(seed=42).select(range(10)).map(add_speech_features)\n",
    "\n",
    "# Unfortunately, HF dataset's audio decoding does not account for soundfile throwing exceptions\n",
    "import io\n",
    "import soundfile as sf\n",
    "def soundfile_validate_filter(example):\n",
    "    try:\n",
    "        b = io.BytesIO(example['audio']['bytes'])\n",
    "        array, sr = sf.read(b)\n",
    "        return True\n",
    "    except sf.LibsndfileError as e:\n",
    "        return False\n",
    "\n",
    "theidx = 0\n",
    "def test_map(example, idx):\n",
    "    global theidx\n",
    "    theidx = idx\n",
    "    return example['speaker'] in qualified_speakers\n",
    "total_examples = len(train_data)\n",
    "reversed_indices = list(range(total_examples - 1, -1, -1))\n",
    "\n",
    "filtered_dataset = train_data.select(reversed_indices).cast_column(\n",
    "    'audio', Audio(decode=False)).filter(\n",
    "    soundfile_validate_filter).filter(\n",
    "    test_map, with_indices=True, num_proc=1).cast_column(\n",
    "    'audio', Audio(decode=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc10ce5f0454ec8862213f40b9c4891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/59603 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b44cd8c91de640c59890488e1b80db93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/597 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "39293853130"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset\n",
    "\n",
    "dataset_with_features = filtered_dataset.map(add_speech_features)\n",
    "dataset_with_features = dataset_with_features.remove_columns(['audio'])\n",
    "\n",
    "dataset_with_features.to_parquet('dataset_over5min.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f968db500b54aab80f8621c5e08da12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'DatasetDict' object has no attribute 'to_parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m dataset_with_features \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m'\u001b[39m, data_files\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_over5min.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m dataset_with_features \u001b[38;5;241m=\u001b[39m dataset_with_features\u001b[38;5;241m.\u001b[39mremove_columns([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 4\u001b[0m \u001b[43mdataset_with_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_over5min_noaudio.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DatasetDict' object has no attribute 'to_parquet'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_with_features = load_dataset('parquet', data_files=['dataset_over5min.parquet'])['train']\n",
    "dataset_with_features = dataset_with_features.remove_columns(['audio'])\n",
    "dataset_with_features.to_parquet('dataset_over5min_noaudio.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
