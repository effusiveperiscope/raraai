{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download audio dataset, set up models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c38099680db46ca80a41444e3cc596d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1629722a6c4945a7836d179c17257c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10787be43fd74bed8511c96ceda749fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-27 00:03:27 | WARNING | xformers | WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.0+cu118 with CUDA 1108 (you have 2.3.1+cu121)\n",
      "    Python  3.10.11 (you have 3.10.7)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "2024-07-27 00:03:27 | WARNING | xformers | A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n",
      "2024-07-27 00:03:27 | WARNING | xformers | Triton is not available, some optimizations will not be enabled.\n",
      "This is just a warning: triton is not available\n",
      "2024-07-27 00:03:28 | INFO | fairseq.tasks.hubert_pretraining | current directory is d:\\Code\\raraai\\2_featureexpl\n",
      "2024-07-27 00:03:28 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'metadata', 'fine_tuning': False, 'labels': ['km'], 'label_dir': 'label', 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2024-07-27 00:03:28 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': True, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}\n",
      "c:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalize: False\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "dataset = load_dataset(\"synthbot/pony-speech\")\n",
    "train_data = dataset['train']\n",
    "\n",
    "from svc_helper.sfeatures.models import RVCHubertModel\n",
    "import torch\n",
    "sfeatures_model = RVCHubertModel(device = torch.device('cuda'))\n",
    "\n",
    "\n",
    "import librosa\n",
    "def add_speech_features(example):\n",
    "    audio = example['audio']['array']\n",
    "    audio_resamp = librosa.resample(audio,\n",
    "        orig_sr=example['audio']['sampling_rate'],\n",
    "        target_sr=RVCHubertModel.expected_sample_rate)\n",
    "    audio_padded = sfeatures_model.pad_audio(audio_resamp)\n",
    "    feats = sfeatures_model.extract_features(audio=\n",
    "        torch.from_numpy(audio_padded))\n",
    "    example['rvc_features'] = feats.cpu().numpy()\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Select speakers and extract speech features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dill\\_dill.py:1705: PicklingWarning: Cannot locate reference to <enum 'Choices'>.\n",
      "  warnings.warn('Cannot locate reference to %r.' % (obj,), PicklingWarning)\n",
      "c:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dill\\_dill.py:1707: PicklingWarning: Cannot pickle <enum 'Choices'>: fairseq.dataclass.constants.Choices has recursive self-references that trigger a RecursionError.\n",
      "  warnings.warn('Cannot pickle %r: %s.%s has recursive self-references that trigger a RecursionError.' % (obj, obj.__module__, obj_name), PicklingWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d6237c411049868b80dd5df576af96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77ebae72f9442f08c53bb2e974a3e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=16):   0%|          | 0/64659 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d1d94db1314173bc7d88905cf1d8ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "speakers = ['Rarity', 'Pinkie Pie']\n",
    "speakers_data = {}\n",
    "n_data = 50\n",
    "for speaker in speakers:\n",
    "    speakers_data[speaker] = train_data.filter(lambda ex, speaker=speaker:\n",
    "        ex['speaker']==speaker, num_proc=16).shuffle().select(range(n_data)).map(\n",
    "            add_speech_features\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Calculate mean over each feature, over speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a4b3911228d4ead938704e65ad6e20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63bbe490941401ca47e6c53c61e2993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def get_features_stats(row):\n",
    "    data = row['rvc_features']\n",
    "    row['rvc_features_mean'] = np.mean(data, axis=1)\n",
    "    return row\n",
    "\n",
    "def get_stats(subset):\n",
    "    summary = subset.map(get_features_stats)\n",
    "    mean = np.array(summary['rvc_features_mean']).squeeze().mean(axis=0)\n",
    "    return mean\n",
    "\n",
    "speaker_stats = {}\n",
    "for speaker in speakers_data.keys():\n",
    "    stats = get_stats(speakers_data[speaker])\n",
    "    speaker_stats[speaker] = stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Prepare RVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "624caddbf67b44e8a4845210c6dfb374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PinkieS1.pth:   0%|          | 0.00/57.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379f89150bb24a2194d1bfc908cf7189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)_IVF1260_Flat_nprobe_1_PinkieS1_v2.index:   0%|          | 0.00/155M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-27 00:14:39 | INFO | svc_helper.svc.rvc.modules.vc.modules | Get sid: D:\\hf_cache\\hub\\models--therealvul--RVCv2\\snapshots\\87778762d011892db45370f0dd963be836d55a08\\PinkiePieS1\\PinkieS1.pth\n",
      "2024-07-27 00:14:39 | INFO | svc_helper.svc.rvc.modules.vc.modules | Loading: D:\\hf_cache\\hub\\models--therealvul--RVCv2\\snapshots\\87778762d011892db45370f0dd963be836d55a08\\PinkiePieS1\\PinkieS1.pth\n",
      "c:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from huggingface_hub import hf_hub_download\n",
    "from svc_helper.svc.rvc import RVCModel\n",
    "import torch\n",
    "\n",
    "rvc_model = RVCModel()\n",
    "# test_model_path = hf_hub_download(repo_id='therealvul/RVCv2', \n",
    "#     filename='RarityS1/Rarity.pth')\n",
    "# test_index_path = hf_hub_download(repo_id='therealvul/RVCv2', \n",
    "#     filename='RarityS1/added_IVF1866_Flat_nprobe_1_Rarity_v2.index')\n",
    "test_model_path = hf_hub_download(repo_id='therealvul/RVCv2', \n",
    "    filename='PinkiePieS1/PinkieS1.pth')\n",
    "test_index_path = hf_hub_download(repo_id='therealvul/RVCv2', \n",
    "    filename='PinkiePieS1/added_IVF1260_Flat_nprobe_1_PinkieS1_v2.index')\n",
    "rvc_model.load_model(model_path = test_model_path,\n",
    "    index_path = test_index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0006, device='cuda:0', dtype=torch.float64)\n",
      "tensor(-0.0083, device='cuda:0', dtype=torch.float64)\n",
      "tensor(-0.0083, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import IPython.display as ipd\n",
    "import soundfile as sf\n",
    "input_path = 'test_speech_2.wav'\n",
    "def force_to_mean(features, speaker='Pinkie Pie', a=1.0):\n",
    "    input_features_mean = torch.mean(features, dim=1)\n",
    "    target_mean = torch.from_numpy(speaker_stats[speaker]).to(features.device)\n",
    "    delta_mean = target_mean - input_features_mean\n",
    "    #print(torch.sum(delta_mean*a))\n",
    "    print(torch.mean(delta_mean*a))\n",
    "    print(torch.mean(target_mean))\n",
    "    print(torch.mean(features + delta_mean*a))\n",
    "    #print(delta_mean.shape)\n",
    "    return features + delta_mean*a\n",
    "\n",
    "wav_opt = rvc_model.infer_file(input_path, transpose=14,\n",
    "    feature_transform=lambda feat: force_to_mean(feat), index_rate=0.75)\n",
    "ipd.Audio(wav_opt, rate=rvc_model.output_sample_rate())\n",
    "sf.write('pinkie_a5_ir0.75.wav', data=wav_opt, samplerate=48000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
