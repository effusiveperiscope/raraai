{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Obtaining decoder states from Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m processor \u001b[38;5;241m=\u001b[39m WhisperProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/whisper-base.en\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m WhisperForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/whisper-base.en\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m audio, samplerate \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maudio.flac\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m input_features \u001b[38;5;241m=\u001b[39m processor(\n\u001b[0;32m      7\u001b[0m     audio, sampling_rate\u001b[38;5;241m=\u001b[39msamplerate, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m )\u001b[38;5;241m.\u001b[39minput_features\n\u001b[0;32m      9\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_features, \n\u001b[0;32m     10\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m#output_scores=True,\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     return_dict_in_generate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\audio.py:189\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# Final cleanup for dtype and contiguity\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mono:\n\u001b[1;32m--> 189\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mto_mono\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    192\u001b[0m     y \u001b[38;5;241m=\u001b[39m resample(y, orig_sr\u001b[38;5;241m=\u001b[39msr_native, target_sr\u001b[38;5;241m=\u001b[39msr, res_type\u001b[38;5;241m=\u001b[39mres_type)\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\audio.py:504\u001b[0m, in \u001b[0;36mto_mono\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;124;03m\"\"\"Convert an audio signal to mono by averaging samples across channels.\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \n\u001b[0;32m    480\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;124;03m(117601,)\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;66;03m# Validate the buffer.  Stereo is ok here.\u001b[39;00m\n\u001b[1;32m--> 504\u001b[0m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalid_audio\u001b[49m(y, mono\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    507\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(y, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mrange\u001b[39m(y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)))\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lazy_loader\\__init__.py:77\u001b[0m, in \u001b[0;36mattach.<locals>.__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m attr_to_modules:\n\u001b[0;32m     76\u001b[0m     submod_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_to_modules[name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 77\u001b[0m     submod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubmod_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(submod, name)\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# If the attribute lives in a file (module) with the same\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# name as the attribute, ensure that the attribute and *not*\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# the module is accessible on the package.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\util\\utils.py:1063\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[38;5;124;03m\"\"\"Numba stencil for local minima computation\"\"\"\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m x[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m&\u001b[39m (x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;129;43m@numba\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguvectorize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvoid(int16[:], bool_[:])\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvoid(int32[:], bool_[:])\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvoid(int64[:], bool_[:])\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvoid(float32[:], bool_[:])\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvoid(float64[:], bool_[:])\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m(n)->(n)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnopython\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m-> 1063\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m_localmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pragma: no cover\u001b[39;49;00m\n\u001b[0;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43;03m\"\"\"Vectorized wrapper for the localmax stencil\"\"\"\u001b[39;49;00m\n\u001b[0;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_localmax_sten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numba\\np\\ufunc\\decorators.py:201\u001b[0m, in \u001b[0;36mguvectorize.<locals>.wrap\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap\u001b[39m(func):\n\u001b[1;32m--> 201\u001b[0m     guvec \u001b[38;5;241m=\u001b[39m GUVectorize(func, signature, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fty \u001b[38;5;129;01min\u001b[39;00m ftylist:\n\u001b[0;32m    203\u001b[0m         guvec\u001b[38;5;241m.\u001b[39madd(fty)\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numba\\np\\ufunc\\decorators.py:56\u001b[0m, in \u001b[0;36mGUVectorize.__new__\u001b[1;34m(cls, func, signature, **kws)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m imp \u001b[38;5;129;01mis\u001b[39;00m gufunc\u001b[38;5;241m.\u001b[39mGUFunc:\n\u001b[0;32m     55\u001b[0m     is_dyn \u001b[38;5;241m=\u001b[39m kws\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_dynamic\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midentity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midentity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m               \u001b[49m\u001b[43mis_dynamic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_dyn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetoptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkws\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m               \u001b[49m\u001b[43mwritable_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwritable_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m imp(func, signature, identity\u001b[38;5;241m=\u001b[39midentity, cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[0;32m     61\u001b[0m                targetoptions\u001b[38;5;241m=\u001b[39mkws, writable_args\u001b[38;5;241m=\u001b[39mwritable_args)\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numba\\np\\ufunc\\gufunc.py:28\u001b[0m, in \u001b[0;36mGUFunc.__init__\u001b[1;34m(self, py_func, signature, identity, cache, is_dynamic, targetoptions, writable_args)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_identity \u001b[38;5;241m=\u001b[39m identity\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# GUFunc cannot inherit from GUFuncBuilder because \"identity\"\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# is a property of GUFunc. Thus, we hold a reference to a GUFuncBuilder\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# object here\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgufunc_builder \u001b[38;5;241m=\u001b[39m \u001b[43mGUFuncBuilder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpy_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midentity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetoptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwritable_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgufunc_builder\u001b[38;5;241m.\u001b[39mpy_func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m     31\u001b[0m functools\u001b[38;5;241m.\u001b[39mupdate_wrapper(\u001b[38;5;28mself\u001b[39m, py_func)\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numba\\np\\ufunc\\ufuncbuilder.py:346\u001b[0m, in \u001b[0;36mGUFuncBuilder.__init__\u001b[1;34m(self, py_func, signature, identity, cache, targetoptions, writable_args)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midentity \u001b[38;5;241m=\u001b[39m parse_identity(identity)\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _suppress_deprecation_warning_nopython_not_supplied():\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnb_func \u001b[38;5;241m=\u001b[39m \u001b[43mjit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnpyufunc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m(py_func)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature \u001b[38;5;241m=\u001b[39m signature\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msout \u001b[38;5;241m=\u001b[39m parse_signature(signature)\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numba\\core\\decorators.py:173\u001b[0m, in \u001b[0;36mjit\u001b[1;34m(signature_or_function, locals, cache, pipeline_class, boundscheck, **options)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe keyword argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnopython=False\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m was supplied. From \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    169\u001b[0m                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumba 0.59.0 the default is being changed to True and use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnopython=False\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m will raise a warning as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument will have no effect. See \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetails.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 173\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[43mNumbaDeprecationWarning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    175\u001b[0m options[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboundscheck\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m boundscheck\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# Handle signature\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numba\\core\\errors.py:40\u001b[0m, in \u001b[0;36mNumbaWarning.__init__\u001b[1;34m(self, msg, loc, highlighting)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28msuper\u001b[39m(NumbaWarning, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     38\u001b[0m         highlight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (msg, loc\u001b[38;5;241m.\u001b[39mstrformat())))\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28msuper\u001b[39m(NumbaWarning, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mhighlight\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numba\\core\\errors.py:317\u001b[0m, in \u001b[0;36mHighlightColorScheme.errmsg\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merrmsg\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg):\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_markup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_errmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numba\\core\\errors.py:307\u001b[0m, in \u001b[0;36mHighlightColorScheme._markup\u001b[1;34m(self, msg, color, style)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m style:\n\u001b[0;32m    306\u001b[0m     features \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m style\n\u001b[1;32m--> 307\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mColorShell\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m reset_terminal() \u001b[38;5;28;01mas\u001b[39;00m mu:\n\u001b[0;32m    309\u001b[0m         mu \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numba\\core\\errors.py:225\u001b[0m, in \u001b[0;36mColorShell.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 225\u001b[0m     \u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_initialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\colorama\\initialise.py:52\u001b[0m, in \u001b[0;36minit\u001b[1;34m(autoreset, convert, strip, wrap)\u001b[0m\n\u001b[0;32m     49\u001b[0m     wrapped_stdout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstdout \u001b[38;5;241m=\u001b[39m wrapped_stdout \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m---> 52\u001b[0m         \u001b[43mwrap_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_stdout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoreset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     wrapped_stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\colorama\\initialise.py:113\u001b[0m, in \u001b[0;36mwrap_stream\u001b[1;34m(stream, convert, strip, autoreset, wrap)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_stream\u001b[39m(stream, convert, strip, autoreset, wrap):\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap:\n\u001b[1;32m--> 113\u001b[0m         wrapper \u001b[38;5;241m=\u001b[39m \u001b[43mAnsiToWin32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m wrapper\u001b[38;5;241m.\u001b[39mshould_wrap():\n\u001b[0;32m    116\u001b[0m             stream \u001b[38;5;241m=\u001b[39m wrapper\u001b[38;5;241m.\u001b[39mstream\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\colorama\\ansitowin32.py:96\u001b[0m, in \u001b[0;36mAnsiToWin32.__init__\u001b[1;34m(self, wrapped, convert, strip, autoreset)\u001b[0m\n\u001b[0;32m     91\u001b[0m on_windows \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# We test if the WinAPI works, because even if we are on Windows\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# we may be using a terminal that doesn't support the WinAPI\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# (e.g. Cygwin Terminal). In this case it's up to the terminal\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# to support the ANSI codes.\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m conversion_supported \u001b[38;5;241m=\u001b[39m on_windows \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mwinapi_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m     fd \u001b[38;5;241m=\u001b[39m wrapped\u001b[38;5;241m.\u001b[39mfileno()\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\colorama\\win32.py:115\u001b[0m, in \u001b[0;36mwinapi_test\u001b[1;34m()\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwinapi_test\u001b[39m():\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_winapi_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\n\u001b[0;32m    116\u001b[0m \u001b[43m               \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_GetStdHandle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSTDOUT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_GetStdHandle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSTDERR\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\colorama\\win32.py:115\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwinapi_test\u001b[39m():\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[43m_winapi_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m    116\u001b[0m                (_GetStdHandle(STDOUT), _GetStdHandle(STDERR)))\n",
      "File \u001b[1;32mc:\\Users\\vul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\colorama\\win32.py:110\u001b[0m, in \u001b[0;36m_winapi_test\u001b[1;34m(handle)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_winapi_test\u001b[39m(handle):\n\u001b[0;32m    109\u001b[0m     csbi \u001b[38;5;241m=\u001b[39m CONSOLE_SCREEN_BUFFER_INFO()\n\u001b[1;32m--> 110\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[43m_GetConsoleScreenBufferInfo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsbi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(success)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import librosa\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base.en\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base.en\")\n",
    "audio, samplerate = librosa.load('audio.flac', sr=16000)\n",
    "input_features = processor(\n",
    "    audio, sampling_rate=samplerate, return_tensors='pt'\n",
    ").input_features\n",
    "output = model.generate(input_features, \n",
    "    output_hidden_states=True, \n",
    "    #output_scores=True,\n",
    "    return_dict_in_generate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "10\n",
      "tensor([[[-0.0091, -0.0120,  0.0026,  ...,  0.0362, -0.0025, -0.0236],\n",
      "         [ 0.0030,  0.0013,  0.0040,  ...,  0.0098, -0.0004,  0.0036],\n",
      "         [-0.0014, -0.0302,  0.0142,  ..., -0.0119, -0.0084, -0.0049],\n",
      "         ...,\n",
      "         [ 0.0043, -0.0053,  0.0097,  ...,  0.0007, -0.0099, -0.0059],\n",
      "         [-0.0196, -0.0066, -0.0132,  ..., -0.0010,  0.0002, -0.0069],\n",
      "         [-0.0015, -0.0299,  0.0106,  ...,  0.0053, -0.0031,  0.0013]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "sm = torch.nn.Softmax(dim=-1)\n",
    "print(len(output.encoder_hidden_states))\n",
    "print(len(output.decoder_hidden_states))\n",
    "print(torch.cat([t[0] for t in output.decoder_hidden_states], dim=1))\n",
    "#print(sm(output.scores[0]))\n",
    "#output.scores[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "lol = nn.Linear(51864, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How GlowTTS's alignment works (training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import monotonic_align\n",
    "import math\n",
    "\n",
    "# This function is explained in addendum 2.A\n",
    "def sequence_mask(length, max_length=None):\n",
    "  if max_length is None:\n",
    "    max_length = length.max()\n",
    "  x = torch.arange(max_length, dtype=length.dtype, device=length.device)\n",
    "  return x.unsqueeze(0) < length.unsqueeze(1)\n",
    "\n",
    "def generate_path(duration, mask):\n",
    "  \"\"\"\n",
    "  duration: [b, t_x]\n",
    "  mask: [b, t_x, t_y]\n",
    "  \"\"\"\n",
    "  device = duration.device\n",
    "  \n",
    "  b, t_x, t_y = mask.shape\n",
    "  cum_duration = torch.cumsum(duration, 1)\n",
    "  path = torch.zeros(b, t_x, t_y, dtype=mask.dtype).to(device=device)\n",
    "  \n",
    "  cum_duration_flat = cum_duration.view(b * t_x)\n",
    "  path = sequence_mask(cum_duration_flat, t_y).to(mask.dtype)\n",
    "  path = path.view(b, t_x, t_y)\n",
    "  path = path * ~F.pad(path, convert_pad_shape([[0, 0], [1, 0], [0, 0]]))[:,:-1]\n",
    "  path = path * mask\n",
    "  return path\n",
    "\n",
    "def convert_pad_shape(pad_shape):\n",
    "  l = pad_shape[::-1]\n",
    "  pad_shape = [item for sublist in l for item in sublist]\n",
    "  return pad_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a set of features `x` where `x.shape == (b=2,n=4,c=5)`, \n",
    "where `b=2` is the minibatch size, `n=4` is the sequence length, and `c=5` is the feature dimension.\n",
    "\n",
    "We wish to align this against another set of features of longer length `z` where `z.shape == (b=1,n=6,c=7`.\n",
    "\n",
    "To distinguish between the sequence length of `x` and `z` we will refer to them as `n_x` and `n_z` respectively. We will do the same for the feature dimension `c_x` and `c_z`.\n",
    "\n",
    "* Note that the feature dimension needs to stay the same; we can only perform alignment on a single dimension.\n",
    "\n",
    "* Also note: the alignment method here is formulated only such that one x-feature can be aligned to one or more z-features (i.e. there are assumed to be more z-features than x-features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 5])\n",
      "torch.Size([2, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[ # (The numbers are not meaningful.)\n",
    "    [0.0, 1.0, 1.0, 1.0, 0.0],\n",
    "    [1.0, 0.5, 0.2, 0.3, 0.6],\n",
    "    [0.4, 0.2, 0.9, 0.1, 0.1],\n",
    "    [0.4, 0.6, 0.7, 0.8, 0.1],\n",
    "]]) # Each row represents an individual feature; there are 4 features in x.\n",
    "\n",
    "z = torch.Tensor([[\n",
    "    [0.0, 1.0, 1.0, 1.0, 0.0, 0.2, 0.1],\n",
    "    [1.0, 0.5, 0.2, 0.3, 0.6, 0.3, 0.1],\n",
    "    [0.4, 0.2, 0.9, 0.1, 0.1, 0.4, 0.1],\n",
    "    [0.4, 0.6, 0.7, 0.8, 0.1, 0.5, 0.1],\n",
    "    [0.5, 0.7, 0.2, 0.1, 0.3, 0.1, 0.1],\n",
    "    [0.9, 0.8, 0.6, 0.5, 0.3, 0.3, 0.1],\n",
    "]]) # And there are 6 features in z.\n",
    "\n",
    "x = torch.cat((x,x), dim=0)\n",
    "z = torch.cat((z,z), dim=0)\n",
    "\n",
    "n_x = x.shape[1]\n",
    "n_z = x.shape[1]\n",
    "c_x = x.shape[2]\n",
    "c_z = z.shape[2]\n",
    "print(x.shape)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GlowTTS monotonic alignment method relies on modeling each feature in `z` as having been sampled from a  Gaussian (aka normal) distribution, with one distribution corresponding to each feature in the input (multiple `z`s may have been sampled from the same distribution).\n",
    "\n",
    "*Remember that a Gaussian distribution can be entirely parameterized by its mean (mu) and standard deviation (sigma).*\n",
    "\n",
    "The GlowTTS text encoder intakes the `n_x` features from x and spits out `n_x` Gaussian distributions; that is, each individual x feature will turn into a pair of mu and sigma.\n",
    "\n",
    "GlowTTS labels the means as `x_m`.\n",
    "\n",
    "*For numerical stability, GlowTTS actually generates the natural logarithm of the standard deviation which it labels as `x_logs`, so we'll use that.\n",
    "\n",
    "In order to sample `z` from these statistics, they must have the same feature dimension `c_z` as `z` itself. \n",
    "\n",
    "So the shape of `x_m` and `x_logs` are both `(b = 1, n = n_x = 4, c = c_z = 6)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: tensor([[[ 0.3989, -0.3484, -0.7459, -0.3408, -0.9484, -0.5320, -0.4384],\n",
      "         [ 0.1561,  0.2102, -0.7977,  0.5547, -0.1628,  0.4119, -0.8767],\n",
      "         [-0.0178,  0.2536, -0.7373,  0.0517, -0.3926, -0.0668, -0.2265],\n",
      "         [ 0.3413, -0.1176, -0.7862,  0.0723, -0.5547, -0.2132, -0.5495]],\n",
      "\n",
      "        [[ 0.3989, -0.3484, -0.7459, -0.3408, -0.9484, -0.5320, -0.4384],\n",
      "         [ 0.1561,  0.2102, -0.7977,  0.5547, -0.1628,  0.4119, -0.8767],\n",
      "         [-0.0178,  0.2536, -0.7373,  0.0517, -0.3926, -0.0668, -0.2265],\n",
      "         [ 0.3413, -0.1176, -0.7862,  0.0723, -0.5547, -0.2132, -0.5495]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "std: tensor([[[ 1.1922, -0.4618, -0.1391,  0.1636,  0.5802,  0.5513, -0.1889],\n",
      "         [ 1.0124, -0.2609, -0.4649, -0.0640,  0.3907,  0.7953, -0.5306],\n",
      "         [ 0.8080, -0.1062, -0.3231,  0.0330,  0.1573,  0.6409, -0.3014],\n",
      "         [ 1.0397, -0.3062, -0.1226,  0.0615,  0.3838,  0.6420, -0.4108]],\n",
      "\n",
      "        [[ 1.1922, -0.4618, -0.1391,  0.1636,  0.5802,  0.5513, -0.1889],\n",
      "         [ 1.0124, -0.2609, -0.4649, -0.0640,  0.3907,  0.7953, -0.5306],\n",
      "         [ 0.8080, -0.1062, -0.3231,  0.0330,  0.1573,  0.6409, -0.3014],\n",
      "         [ 1.0397, -0.3062, -0.1226,  0.0615,  0.3838,  0.6420, -0.4108]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "x_m.shape:  torch.Size([2, 4, 7])\n",
      "x_logs.shape:  torch.Size([2, 4, 7])\n"
     ]
    }
   ],
   "source": [
    "# We simulate the \"text encoder\" with two linear layers for generating the correct feature dimensions.\n",
    "\n",
    "text_encoder_mean = nn.Linear(c_x, c_z)\n",
    "text_encoder_logs = nn.Linear(c_x, c_z)\n",
    "\n",
    "#with torch.no_grad():\n",
    "# I have enabled gradients here so we can see where gradients propagate in the calculation.\n",
    "x_m = text_encoder_mean(x)\n",
    "x_logs = text_encoder_logs(x)\n",
    "\n",
    "print('mean:', x_m)\n",
    "print('std:', x_logs)\n",
    "print('x_m.shape: ',x_m.shape)\n",
    "print('x_logs.shape: ',x_logs.shape)\n",
    "\n",
    "# Needed for following step\n",
    "x_m = x_m.transpose(1,2)\n",
    "x_logs = x_logs.transpose(1,2)\n",
    "z = z.transpose(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have obtained `n_x` Gaussian distributions. We somehow need to map these into our `n_z` features.\n",
    "\n",
    "To do this, we produce a \"likelihood score matrix\". For each feature in `z`, we calculate the Gaussian probability density function against ALL of the distributions obtained from the text encoder--that is, against ALL the pairs of mu and sigma. Each cell in the matrix represents the probability that the `z` feature for that cell could have been sampled from the `x_m` and `x_logs` associated with that cell.\n",
    "\n",
    "Since there are `n_x` Gaussian distributions and `n_z` z-features, this results in an `n_x` by `n_z` likelihood score matrix.\n",
    "\n",
    "(Actually, it calculates the log PDF instead of the PDF directly--but again that's not very important.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7, 4])\n",
      "torch.Size([2, 4, 1])\n",
      "torch.Size([2, 4, 6])\n",
      "torch.Size([2, 4, 6])\n",
      "torch.Size([2, 4, 1])\n",
      "tensor([[[-13.5290, -10.4938, -10.8956, -11.6728, -10.6972, -11.8238],\n",
      "         [-13.4338, -10.2392, -12.4779, -11.7136, -10.3365, -11.5456],\n",
      "         [-11.1495,  -8.8243, -10.1350,  -9.8962,  -8.6065,  -9.7008],\n",
      "         [-11.8729,  -9.5688, -10.2613, -10.4900,  -9.6226, -10.5103]],\n",
      "\n",
      "        [[-13.5290, -10.4938, -10.8956, -11.6728, -10.6972, -11.8238],\n",
      "         [-13.4338, -10.2392, -12.4779, -11.7136, -10.3365, -11.5456],\n",
      "         [-11.1495,  -8.8243, -10.1350,  -9.8962,  -8.6065,  -9.7008],\n",
      "         [-11.8729,  -9.5688, -10.2613, -10.4900,  -9.6226, -10.5103]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([2, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "# The below code is a bit convoluted, but just trust that it calculates a log Gaussian PDF.\n",
    "x_s_sq_r = torch.exp(-2 * x_logs) # [b, d, t]\n",
    "print(x_s_sq_r.shape) # [b, d, t]\n",
    "logp1 = torch.sum(-0.5 * math.log(2 * math.pi) - x_logs, [1]).unsqueeze(-1) # [b, t, 1]\n",
    "print(logp1.shape)\n",
    "logp2 = torch.matmul(x_s_sq_r.transpose(1,2), -0.5 * (z ** 2)) # [b, t, d] x [b, d, t'] = [b, t, t']\n",
    "print(logp2.shape)\n",
    "logp3 = torch.matmul((x_m * x_s_sq_r).transpose(1,2), z) # [b, t, d] x [b, d, t'] = [b, t, t']\n",
    "print(logp3.shape)\n",
    "logp4 = torch.sum(-0.5 * (x_m ** 2) * x_s_sq_r, [1]).unsqueeze(-1) # [b, t, 1]\n",
    "print(logp4.shape)\n",
    "logp = logp1 + logp2 + logp3 + logp4 # [b, t, t']\n",
    "print(logp)\n",
    "print(logp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use monotonic_align.maximum_path to plot a monotonic maximum sum path through the likelihoods, using dynamic programming [(specifically the Viterbi algorithm, which goes by many other names.)](https://en.wikipedia.org/wiki/Viterbi_algorithm). This path follows a few constraints:\n",
    "\n",
    "1. Monotonicity -- each z can only depend on the Gaussian distribution associated with the previous z-feature, or the distribution following that distribution--in other words, the model is only allowed to read \"left to right\". It's not allowed to skip input features or repeat them later, and it should always begin with the first input feature and end on the last feature.\n",
    "2. Maximum path score -- this is the monotonic path, that, according to the text encoder's generated distributions, will sum to maximum likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 4])\n",
      "torch.Size([2, 1, 6])\n",
      "torch.Size([2, 1, 4, 6])\n",
      "tensor([[[[1., 0., 0., 0., 0., 0.],\n",
      "          [0., 1., 0., 0., 0., 0.],\n",
      "          [0., 0., 1., 1., 1., 0.],\n",
      "          [0., 0., 0., 0., 0., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0., 0., 0.],\n",
      "          [0., 1., 0., 0., 0., 0.],\n",
      "          [0., 0., 1., 1., 1., 0.],\n",
      "          [0., 0., 0., 0., 0., 1.]]]])\n",
      "torch.Size([2, 1, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "# This masking is explained in the addendum 2.A\n",
    "x_mask = torch.unsqueeze(sequence_mask(torch.Tensor([4, 4])), 1)\n",
    "z_mask = torch.unsqueeze(sequence_mask(torch.Tensor([6, 6])), 1)\n",
    "attn_mask = torch.unsqueeze(x_mask, -1) * torch.unsqueeze(z_mask, 2)\n",
    "print(x_mask.shape)\n",
    "print(z_mask.shape)\n",
    "print(attn_mask.shape)\n",
    "\n",
    "attn = monotonic_align.maximum_path(logp, attn_mask.squeeze(1)).unsqueeze(1).detach()\n",
    "print(attn)\n",
    "print(attn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row index corresponds to a Gaussian distribution (which in turn corresponds to an x-feature), and each column index corresponds to a z-feature.\n",
    "\n",
    "Now we've obtained our maximum path, all that's left to do is to use our alignment to sub in the distributions corresponding to each z-vector, to get a properly sized set of distributions (columns in the outputted matrix) from which latents can be sampled. This can be done with a simple matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3989,  0.1561, -0.0178, -0.0178, -0.0178,  0.3413],\n",
      "         [-0.3484,  0.2102,  0.2536,  0.2536,  0.2536, -0.1176],\n",
      "         [-0.7459, -0.7977, -0.7373, -0.7373, -0.7373, -0.7862],\n",
      "         [-0.3408,  0.5547,  0.0517,  0.0517,  0.0517,  0.0723],\n",
      "         [-0.9484, -0.1628, -0.3926, -0.3926, -0.3926, -0.5547],\n",
      "         [-0.5320,  0.4119, -0.0668, -0.0668, -0.0668, -0.2132],\n",
      "         [-0.4384, -0.8767, -0.2265, -0.2265, -0.2265, -0.5495]],\n",
      "\n",
      "        [[ 0.3989,  0.1561, -0.0178, -0.0178, -0.0178,  0.3413],\n",
      "         [-0.3484,  0.2102,  0.2536,  0.2536,  0.2536, -0.1176],\n",
      "         [-0.7459, -0.7977, -0.7373, -0.7373, -0.7373, -0.7862],\n",
      "         [-0.3408,  0.5547,  0.0517,  0.0517,  0.0517,  0.0723],\n",
      "         [-0.9484, -0.1628, -0.3926, -0.3926, -0.3926, -0.5547],\n",
      "         [-0.5320,  0.4119, -0.0668, -0.0668, -0.0668, -0.2132],\n",
      "         [-0.4384, -0.8767, -0.2265, -0.2265, -0.2265, -0.5495]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[[ 1.1922,  1.0124,  0.8080,  0.8080,  0.8080,  1.0397],\n",
      "         [-0.4618, -0.2609, -0.1062, -0.1062, -0.1062, -0.3062],\n",
      "         [-0.1391, -0.4649, -0.3231, -0.3231, -0.3231, -0.1226],\n",
      "         [ 0.1636, -0.0640,  0.0330,  0.0330,  0.0330,  0.0615],\n",
      "         [ 0.5802,  0.3907,  0.1573,  0.1573,  0.1573,  0.3838],\n",
      "         [ 0.5513,  0.7953,  0.6409,  0.6409,  0.6409,  0.6420],\n",
      "         [-0.1889, -0.5306, -0.3014, -0.3014, -0.3014, -0.4108]],\n",
      "\n",
      "        [[ 1.1922,  1.0124,  0.8080,  0.8080,  0.8080,  1.0397],\n",
      "         [-0.4618, -0.2609, -0.1062, -0.1062, -0.1062, -0.3062],\n",
      "         [-0.1391, -0.4649, -0.3231, -0.3231, -0.3231, -0.1226],\n",
      "         [ 0.1636, -0.0640,  0.0330,  0.0330,  0.0330,  0.0615],\n",
      "         [ 0.5802,  0.3907,  0.1573,  0.1573,  0.1573,  0.3838],\n",
      "         [ 0.5513,  0.7953,  0.6409,  0.6409,  0.6409,  0.6420],\n",
      "         [-0.1889, -0.5306, -0.3014, -0.3014, -0.3014, -0.4108]]],\n",
      "       grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z_m = torch.matmul(attn.squeeze(1).transpose(1, 2), x_m.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n",
    "z_logs = torch.matmul(attn.squeeze(1).transpose(1, 2), x_logs.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n",
    "print(z_m)\n",
    "print(z_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that for z-features that form a straight row of \"1\"s in the alignment, the corresponding distributions are the same.\n",
    "\n",
    "We can also see that even though we had to detach our data to pass them into the monotonic alignment algorithm, we still have gradients coming from the x_m and x_logs statistics. We can calculate loss based on our z_m, z_logs, and original z by taking the negative log likelihood (negative log of the normal PDF):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(49.5606, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.sum(z_logs) + (\n",
    "    0.5 * torch.sum(torch.exp(-2 * z_logs) * ((z - z_m) ** 2))) + (\n",
    "    0.5*math.log(2*math.pi))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual GlowTTS loss function has other terms relating to generative flows and also averages across the batch and sequence axes. Another component of the GlowTTS network is the duration predictor, whose inner workings are considered out of scope here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At inference time, we're trying to predict the z directly; however, in the TTS task, we don't have enough information to produce the alignment matrix ourselves, so we don't know which text encoder-outputted distributions can be used to sample which output features. GlowTTS generates the alignment matrix using a separate trained component called the \"duration predictor\", which predicts (the logarithm of) the number of output z-features to assign to each x-feature, denoted `w_ceil` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2., 1., 2.]],\n",
      "\n",
      "        [[1., 2., 1., 2.]]], grad_fn=<CeilBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# We simulate the duration predictor with a linear layer.\n",
    "proj_w = nn.Linear(c_x, 1)\n",
    "\n",
    "# The projection layer outputs log(w), the log duration.\n",
    "logw = proj_w(x).transpose(1,2)\n",
    "# We mask the exponentiated output by the input lengths.\n",
    "w = torch.exp(logw) * x_mask \n",
    "# Then we take the ceiling to ensure we produce integers (we cannot assign an \n",
    "# input to a fractional number of outputs)\n",
    "w_ceil = torch.ceil(w) \n",
    "print(w_ceil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the above duration predictor is not very smart it may output the same duration for every single z-feature. Nonetheless, our next step is to produce the alignment matrix, which can be done using the `generate_path` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0., 0., 0., 0.],\n",
      "          [0., 1., 1., 0., 0., 0.],\n",
      "          [0., 0., 0., 1., 0., 0.],\n",
      "          [0., 0., 0., 0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0., 0., 0., 0.],\n",
      "          [0., 1., 1., 0., 0., 0.],\n",
      "          [0., 0., 0., 1., 0., 0.],\n",
      "          [0., 0., 0., 0., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "# y_lengths represents the total z-feature duration of each utterance, clamped to a minimum of 1.\n",
    "y_lengths = torch.clamp_min(torch.sum(w_ceil, [1, 2]), 1).long() # long() is important, as sequence_mask expects integer inputs.\n",
    "z_mask = torch.unsqueeze(sequence_mask(y_lengths), 1)\n",
    "attn_mask = torch.unsqueeze(x_mask, -1) * torch.unsqueeze(z_mask, 2)\n",
    "\n",
    "attn = generate_path(w_ceil.squeeze(1), attn_mask.squeeze(1)).unsqueeze(1).float()\n",
    "print(attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our alignment matrix, we can use it to get \"sub in\" our Gaussian distributions corresponding to each z-feature, as we did in training, and sample the z-features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7, 6])\n",
      "torch.Size([2, 1, 6])\n",
      "tensor([[[ 1.2520, -0.0424, -0.3758, -0.2452,  0.9683, -1.5186, -1.5762],\n",
      "         [-2.8571,  1.3377, -0.2475,  1.1090, -0.9839, -0.8266, -0.5174],\n",
      "         [-3.6631,  0.8135,  0.5757,  1.8449, -1.3828, -0.1302, -0.4160],\n",
      "         [ 0.4235,  0.2687,  0.5241, -2.3097, -0.3391,  3.0892, -0.5596],\n",
      "         [-0.3883, -0.4462, -1.1454, -0.0425, -0.8873,  1.0433, -0.8623],\n",
      "         [-1.6472,  0.4159, -0.6057, -0.8800, -1.3943, -3.0522, -0.4836]],\n",
      "\n",
      "        [[ 5.8681, -0.3183,  0.0129, -1.0627, -1.0012, -1.8601, -0.1235],\n",
      "         [-0.8571,  0.2519, -0.4191,  0.6520, -2.6594,  1.0875, -0.3768],\n",
      "         [ 0.4435,  0.3587, -0.6444, -0.4242,  0.5319, -0.3713,  0.2347],\n",
      "         [ 2.8368,  0.3967, -1.1188, -0.0618,  0.1658, -0.5675, -1.6146],\n",
      "         [-2.2554, -1.0208, -0.8177, -0.6440,  1.0045, -0.3717, -1.0604],\n",
      "         [ 1.4739,  0.1365, -1.5026, -1.1222, -0.0331, -0.0529, -0.5911]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "torch.Size([2, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "z_m = torch.matmul(attn.squeeze(1).transpose(1, 2), x_m.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n",
    "z_logs = torch.matmul(attn.squeeze(1).transpose(1, 2), x_logs.transpose(1, 2)).transpose(1, 2) # [b, t', t], [b, t, d] -> [b, d, t']\n",
    "print(z_m.shape)\n",
    "print(z_mask.shape)\n",
    "z = (z_m + torch.exp(z_logs) * torch.randn_like(z_m)) * z_mask\n",
    "z = z.transpose(1,2)\n",
    "print(z)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addendum 2.A: sequence_mask and attn_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sequence_mask` takes a 1D tensor of intended input lengths,\n",
    "outputting an arrary of binary masks for each length that is True for positions\n",
    "less than that length, or False for positions greater than that length. (This is so we can multiply the binary mask against quantities in calculations, to ensure the model doesn't use any \"accidental information\" beyond the intended length.)\n",
    "\n",
    "As a trivial example, we produce a mask for a single length first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sequence_mask(length, max_length=None):\n",
    "  if max_length is None:\n",
    "    max_length = length.max()\n",
    "  x = torch.arange(max_length, dtype=length.dtype, device=length.device)\n",
    "  return x.unsqueeze(0) < length.unsqueeze(1)\n",
    "  \n",
    "sequence_mask(torch.Tensor([4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at producing a mask for multiple lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True],\n",
       "        [ True, False, False, False]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_mask(torch.Tensor([4,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row (corresponding to the input \"4\") is filled completely,\n",
    "while the second row has only the first cell marked True.\n",
    "\n",
    "Now what about attn_mask?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ True,  True,  True,  True,  True,  True],\n",
      "          [ True,  True,  True,  True,  True,  True],\n",
      "          [ True,  True,  True,  True,  True,  True],\n",
      "          [ True,  True,  True,  True,  True,  True]]],\n",
      "\n",
      "\n",
      "        [[[ True,  True,  True,  True,  True, False],\n",
      "          [False, False, False, False, False, False],\n",
      "          [False, False, False, False, False, False],\n",
      "          [False, False, False, False, False, False]]]])\n"
     ]
    }
   ],
   "source": [
    "x_mask = torch.unsqueeze(sequence_mask(torch.Tensor([4, 1])), 1) # [2, 1, 4]\n",
    "z_mask = torch.unsqueeze(sequence_mask(torch.Tensor([6, 5])), 1) # [2, 1, 6]\n",
    "attn_mask = torch.unsqueeze(x_mask, -1) * torch.unsqueeze(z_mask, 2) # [2, 1, 4, 1] x [2, 1, 1, 6]\n",
    "print(attn_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attn_mask is fed into the alignment algorithm to ensure the model doesn't try to find an alignment for non-existent distributions or z-features.\n",
    "\n",
    "It uses matrix multiplication to generate pairwise attention masks between each x_mask and z_mask. For the first mask in this sequence, corresponding to an x-length of 4 and z-length of 6, we see that all cells are True. In the second mask in this sequence, corresponding to an x-length of 1, we see that only the first row of cells have True."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
